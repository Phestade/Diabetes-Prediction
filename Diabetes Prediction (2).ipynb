{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e089c87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09545a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# suppress warnings from final output\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b1d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'diabetes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe8ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0473f9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1400a487",
   "metadata": {},
   "source": [
    " Display the first few rows of the dataset to get an overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5bd2e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1563ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae73c214",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809d469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics of the data set accessed.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74d08ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_variable_names(dataframe, cat_threshold=10, card_threshold=20):\n",
    "    \"\"\"\n",
    "    Extracts names of categorical, numerical, and categorical but cardinal variables from a dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        dataframe: DataFrame\n",
    "            The input dataframe.\n",
    "        cat_threshold: int, optional\n",
    "            Threshold value for identifying numerical-looking categorical variables.\n",
    "        card_threshold: int, optional\n",
    "            Threshold value for identifying categorical but cardinal variables.\n",
    "\n",
    "    Returns:\n",
    "        cat_vars: list\n",
    "            List of categorical variable names.\n",
    "        num_vars: list\n",
    "            List of numerical variable names.\n",
    "        card_cat_vars: list\n",
    "            List of categorical-looking cardinal variable names.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Categorical columns and categorical but cardinal columns\n",
    "    cat_vars = [col for col in dataframe.columns if dataframe[col].dtype == \"O\"]\n",
    "    num_like_cat_vars = [col for col in dataframe.columns if dataframe[col].nunique() < cat_threshold and\n",
    "                         dataframe[col].dtype != \"O\"]\n",
    "    card_cat_vars = [col for col in dataframe.columns if dataframe[col].nunique() > card_threshold and\n",
    "                     dataframe[col].dtype == \"O\"]\n",
    "    cat_vars = [col for col in cat_vars if col not in card_cat_vars]\n",
    "    cat_vars = cat_vars + num_like_cat_vars\n",
    "\n",
    "    # Numerical columns\n",
    "    num_vars = [col for col in dataframe.columns if dataframe[col].dtype != \"O\"]\n",
    "    num_vars = [col for col in num_vars if col not in num_like_cat_vars]\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"Number of Observations: {dataframe.shape[0]}\")\n",
    "    print(f\"Number of Variables: {dataframe.shape[1]}\")\n",
    "    print(f'Number of Categorical Variables: {len(cat_vars)}')\n",
    "    print(f'Number of Numerical Variables: {len(num_vars)}')\n",
    "    print(f'Number of Cardinal Categorical Variables: {len(card_cat_vars)}')\n",
    "    print(f'Number of Numerical-Looking Categorical Variables: {len(num_like_cat_vars)}')\n",
    "    \n",
    "    return cat_vars, num_vars, card_cat_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5388b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols, num_cols, cat_but_car = extract_variable_names(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47425bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f25c6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1975d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data visualisation in histogram \n",
    "df.hist(figsize = (10,12))    #to check the distribution of the features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf32ad7",
   "metadata": {},
   "source": [
    "Check the distribution of the 'Outcome' variable (diabetes-positive or diabetes-negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f563245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax = sns.countplot(data=df, x=df['Outcome'].replace({0: 'No Diabetes', 1: 'Diabetes'}),\n",
    "                   ax=axs[0])\n",
    "\n",
    "ax.set_xlabel('Outcome', fontsize=14)\n",
    "ax.set_ylabel('Count', fontsize=14)\n",
    "axs[0].set_title(\"Diabetes Count\", fontsize=16)\n",
    "\n",
    "def func(pct, allvals):\n",
    "    absolute = int(np.round(pct / 100. * np.sum(allvals)))\n",
    "    return f\"{pct:.2f}%\\n({absolute:d})\"\n",
    "\n",
    "explode = [0, 0.07]\n",
    "labels = ['No Diabetes', 'Diabetes']\n",
    "\n",
    "ax2 = df['Outcome'].value_counts().plot.pie(explode=explode,shadow=True,\n",
    "                                                 autopct=lambda pct: func(pct, df['Outcome'].value_counts()),\n",
    "                                                 ylabel='', labels=labels,\n",
    "                                                 ax=axs[1], textprops=dict(color=\"black\", size=13))\n",
    "axs[1].set_title(\"Diabetes Percentage\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a9c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Outcome'].value_counts()*100/len(df),'\\n')\n",
    "print(df['Outcome'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea5177f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def target_variable_summary_numeric(dataframe, target_col, numeric_col):\n",
    "    \"\"\"\n",
    "    Calculate and print the summary of a numeric column grouped by the target variable.\n",
    "\n",
    "    Parameters:\n",
    "        dataframe: DataFrame\n",
    "            The input dataframe.\n",
    "        target_col: str\n",
    "            Name of the target variable column.\n",
    "        numeric_col: str\n",
    "            Name of the numeric column for analysis.\n",
    "    \"\"\"\n",
    "    summary = dataframe.groupby(target_col).agg({numeric_col: \"mean\"})\n",
    "    print(summary, end=\"\\n\\n\\n\")\n",
    "    print(\"------------------------------\")\n",
    "\n",
    "# Iterate through numeric columns and generate target variable summaries\n",
    "for column in num_cols:\n",
    "    target_variable_summary_numeric(df, \"Outcome\", column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec9406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Create a heatmap to visualize the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='YlGnBu', fmt=\".2f\", linewidths=.5)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b467955b",
   "metadata": {},
   "source": [
    "_When we examine the graph, we observe that the highest correlation is between the variables of pregnancy-age , glucose-outcome and insulin-skin thickness_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee66f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data distributions between the columns\n",
    "sns.pairplot(df, hue='Outcome', height=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc053cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data distributions between some highly correlated columns\n",
    "\n",
    "# Separate data for diabetic and healthy individuals\n",
    "diabetic_data = df[df.Outcome == 1]\n",
    "healthy_data = df[df.Outcome == 0]\n",
    "\n",
    "# Create a scatter plot to visualize Age vs. Insulin for diabetic and healthy individuals\n",
    "plt.scatter(healthy_data.Age, healthy_data.Pregnancies, color=\"green\", label=\"Healthy\", alpha=0.5)\n",
    "plt.scatter(diabetic_data.Age, diabetic_data.Pregnancies, color=\"red\", label=\"Diabetic\", alpha=0.5)\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Pregnancies\")\n",
    "plt.legend()\n",
    "\n",
    "# Show the scatter plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6336ed",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd0116",
   "metadata": {},
   "source": [
    "### Checking for Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a92c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(dataframe, include_column_names=False):\n",
    "    \"\"\"\n",
    "    Checks for missing values in a DataFrame and displays a summary.\n",
    "\n",
    "    Parameters:\n",
    "        dataframe: DataFrame\n",
    "            The input dataframe.\n",
    "        include_column_names: bool, optional\n",
    "            If True, returns a list of columns with missing values.\n",
    "\n",
    "    Returns:\n",
    "        List of columns with missing values if include_column_names is True.\n",
    "    \"\"\"\n",
    "    # Find columns with missing values\n",
    "    columns_with_missing = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n",
    "    \n",
    "    # Calculate missing value counts and ratios\n",
    "    missing_counts = dataframe[columns_with_missing].isnull().sum().sort_values(ascending=False)\n",
    "    missing_ratios = (dataframe[columns_with_missing].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)\n",
    "    \n",
    "    # Create a summary DataFrame\n",
    "    missing_df = pd.DataFrame({'Missing Count': missing_counts, 'Missing Ratio (%)': np.round(missing_ratios, 2)})\n",
    "    \n",
    "    # Print the summary\n",
    "    print(missing_df, end=\"\\n\")\n",
    "    \n",
    "    # Return list of columns with missing values if requested\n",
    "    if include_column_names:\n",
    "        return columns_with_missing\n",
    "\n",
    "# Call the function to check missing values\n",
    "check_missing_values(df, include_column_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dcd1a2",
   "metadata": {},
   "source": [
    "_When examining the dataset for missing values, our assessment indicated an absence of such values. Nonetheless, upon closer inspection, we identified occurrences of 0 in fields like Glucose, BloodPressure, SkinThickness, Insulin, and BMI. It is implausible for these variables to assume a value of 0. Consequently, we intend to substitute these 0 values with NaN to accurately represent the absence of valid data points._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f8512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_zeros = [col for col in df.columns if (df[col].min() == 0 and col not in [\"Pregnancies\", \"Outcome\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72468ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe15396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns with potential missing values coded as 0\n",
    "columns_with_zeros = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]\n",
    "\n",
    "# Replace 0 values with NaN in the specified columns\n",
    "df[columns_with_zeros] = df[columns_with_zeros].replace(0, np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eba26f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through columns with potential missing values\n",
    "for col in columns_with_zeros:\n",
    "    # Print the count of 0 values in the column\n",
    "    print(f\"{col}: {df.loc[df[col] == 0].shape[0]}\")\n",
    "    \n",
    "    # Replace 0 values with NaN in the column\n",
    "    df[col] = np.where(df[col] == 0, np.nan, df[col])\n",
    "\n",
    "# Call a function to check missing values in the DataFrame\n",
    "check_missing_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd6f444",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f9012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "msno.bar(df, sort=\"ascending\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef941bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba1c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with the median values for each variable.\n",
    "def fill_missing_with_median(var):   \n",
    "    non_null_values = df[df[var].notnull()]\n",
    "    median_values = non_null_values[[var, 'Outcome']].groupby(['Outcome'])[[var]].median().reset_index()\n",
    "    return median_values\n",
    "\n",
    "# Replace incomplete observations with the median values of non-diabetic and diabetic individuals.\n",
    "columns = df.columns\n",
    "columns = columns.drop(\"Outcome\")\n",
    "for column in columns:\n",
    "    median_values = fill_missing_with_median(column)\n",
    "    df.loc[(df['Outcome'] == 0) & (df[column].isnull()), column] = median_values[column][0]\n",
    "    df.loc[(df['Outcome'] == 1) & (df[column].isnull()), column] = median_values[column][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04091ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the remaining number of missing values\n",
    "missing_values_count = df.isnull().sum()\n",
    "print(\"Number of Missing Values:\")\n",
    "print(missing_values_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a99135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fa2503",
   "metadata": {},
   "source": [
    "### Dealing with Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0effd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outlier_thresholds(dataframe, variable, lower_quantile=0.10, upper_quantile=0.90):\n",
    "    \"\"\"\n",
    "    Calculate the lower and upper outlier thresholds based on quantiles.\n",
    "\n",
    "    Parameters:\n",
    "        dataframe: DataFrame\n",
    "            The input dataframe.\n",
    "        variable: str\n",
    "            The name of the variable for which outlier thresholds are calculated.\n",
    "        lower_quantile: float, optional\n",
    "            The lower quantile value.\n",
    "        upper_quantile: float, optional\n",
    "            The upper quantile value.\n",
    "\n",
    "    Returns:\n",
    "        lower_limit: float\n",
    "            The lower outlier threshold.\n",
    "        upper_limit: float\n",
    "            The upper outlier threshold.\n",
    "    \"\"\"\n",
    "    quantile_one = dataframe[variable].quantile(lower_quantile)\n",
    "    quantile_three = dataframe[variable].quantile(upper_quantile)\n",
    "    interquantile_range = quantile_three - quantile_one\n",
    "    upper_limit = quantile_three + 1.5 * interquantile_range\n",
    "    lower_limit = quantile_one - 1.5 * interquantile_range\n",
    "    return lower_limit, upper_limit\n",
    "\n",
    "def has_outliers(dataframe, col_name):\n",
    "    \"\"\"\n",
    "    Check if a column in the dataframe has outliers.\n",
    "\n",
    "    Parameters:\n",
    "        dataframe: DataFrame\n",
    "            The input dataframe.\n",
    "        col_name: str\n",
    "            The name of the column to check for outliers.\n",
    "\n",
    "    Returns:\n",
    "        bool\n",
    "            True if outliers are present, False otherwise.\n",
    "    \"\"\"\n",
    "    lower_limit, upper_limit = get_outlier_thresholds(dataframe, col_name)\n",
    "    if dataframe[(dataframe[col_name] > upper_limit) | (dataframe[col_name] < lower_limit)].any(axis=None):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Iterate through numeric columns and check for outliers\n",
    "for column in num_cols:\n",
    "    has_outliers_flag = has_outliers(df, column)\n",
    "    print(f\"{column}: {has_outliers_flag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762bf6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with specified size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create a boxplot for all columns\n",
    "df.boxplot()\n",
    "\n",
    "# Add title and adjust x-axis labels\n",
    "plt.title(\"Boxplot of All Columns with Outliers\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Display the boxplot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade3c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_outliers_with_thresholds(dataframe, column):\n",
    "    \"\"\"\n",
    "    Replace outliers in a column of a dataframe with predefined thresholds.\n",
    "\n",
    "    Parameters:\n",
    "        dataframe: DataFrame\n",
    "            The input dataframe.\n",
    "        column: str\n",
    "            The name of the column to replace outliers.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    low_limit, up_limit = get_outlier_thresholds(dataframe, column)\n",
    "    \n",
    "    # Replace outliers with threshold values\n",
    "    dataframe.loc[(dataframe[column] < low_limit), column] = low_limit\n",
    "    dataframe.loc[(dataframe[column] > up_limit), column] = up_limit\n",
    "\n",
    "# Iterate through numeric columns and replace outliers\n",
    "for column in num_cols:\n",
    "    replace_outliers_with_thresholds(df, column)\n",
    "    print(f\"Outliers replaced for {column}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ca668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for outliers\n",
    "for column in num_cols:\n",
    "    has_outliers_flag = has_outliers(df, column)\n",
    "    print(f\"{column}: {has_outliers_flag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2c7511",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aebbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a categorical variable based on BMI ranges\n",
    "bmi_categories = pd.cut(df['BMI'], bins=[0, 18.5, 24.9, 29.9, 34.9, 39.9, float('inf')],\n",
    "                        labels=[\"Underweight\", \"Normal\", \"Overweight\", \"Obesity 1\", \"Obesity 2\", \"Obesity 3\"])\n",
    "df['BMI_CAT'] = bmi_categories\n",
    "\n",
    "# Define a function to set insulin score\n",
    "def set_insulin(row):\n",
    "    if 16 <= row[\"Insulin\"] <= 166:\n",
    "        return \"Normal\"\n",
    "    else:\n",
    "        return \"Abnormal\"\n",
    "\n",
    "# Apply the function to create a new categorical variable for insulin\n",
    "df['Insulin_CAT'] = df.apply(set_insulin, axis=1)\n",
    "\n",
    "# Define BloodPressure categories\n",
    "bp_bins = [0, 60, 80, 90, 120, np.inf]\n",
    "bp_labels = [\"Low_Blood_Pressure\", \"Normal\", \"Prehypertension\", \"Hypertension\", \"Hypertensive_Crisis\"]\n",
    "\n",
    "# Create 'BloodPressure_CAT' categorical variable based on BloodPressure values\n",
    "df['BloodPressure_CAT'] = pd.cut(df['BloodPressure'], bins=bp_bins, labels=bp_labels)\n",
    "\n",
    "# Create a categorical variable based on glucose levels\n",
    "glucose_categories = pd.cut(df['Glucose'], bins=[0, 70, 99, 126, float('inf')],\n",
    "                            labels=[\"Low\", \"Normal\", \"Overweight\", \"High\"])  # Removed \"Secret\" category\n",
    "df['Glucose_CAT'] = glucose_categories\n",
    "\n",
    "# Create age categories using quantiles\n",
    "df['Age_CAT'] = pd.qcut(df['Age'], q=3, labels=[\"Young\", \"Mature\", \"Old\"])\n",
    "\n",
    "# Create a categorical variable for pregnancies\n",
    "df['Preg_CAT'] = pd.cut(df['Pregnancies'], bins=[-1, 0, 1, float('inf')],\n",
    "                        labels=[\"Never\", \"One_Time\", \"Many_Times\"])\n",
    "\n",
    "# Print a message to indicate completion of feature engineering\n",
    "print(\"Feature engineering completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e12a338",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5fb7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols, num_cols, cat_but_car = extract_variable_names(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d816abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of new categorical columns to visualize\n",
    "new_cols = [\"Insulin_CAT\", \"BloodPressure_CAT\", \"Glucose_CAT\", \"BMI_CAT\", \"Age_CAT\", \"Preg_CAT\"]\n",
    "\n",
    "# Define subplot grid parameters\n",
    "rows = 3\n",
    "cols = 2\n",
    "\n",
    "# Calculate the total number of subplots\n",
    "total_subplots = rows * cols\n",
    "\n",
    "# Create a new figure with specified size\n",
    "fig = plt.figure(figsize=(15, 11))\n",
    "\n",
    "# Loop through each new categorical column\n",
    "for subplot_counter, col in enumerate(new_cols, start=1):\n",
    "    if subplot_counter <= total_subplots:\n",
    "        # Create a subplot\n",
    "        plt.subplot(rows, cols, subplot_counter)\n",
    "        \n",
    "        # Create a countplot with hue (Outcome)\n",
    "        ax = sns.countplot(data=df,\n",
    "                           x=col,\n",
    "                           hue=\"Outcome\")\n",
    "        \n",
    "        # Set labels for axes\n",
    "        plt.ylabel('Count')\n",
    "        plt.xlabel(f'{col}', size=15)\n",
    "\n",
    "# Adjust layout and spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496bd6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Label Encoding for binary columns\n",
    "def label_encode_binary(dataframe, column):\n",
    "    label_encoder = LabelEncoder()\n",
    "    dataframe[column] = label_encoder.fit_transform(dataframe[column])\n",
    "    return dataframe\n",
    "\n",
    "# Get list of binary columns\n",
    "binary_columns = [col for col in df.columns if df[col].dtype == \"object\" and df[col].nunique() == 2]\n",
    "\n",
    "# Apply label encoding to binary columns\n",
    "for col in binary_columns:\n",
    "    df = label_encode_binary(df, col)\n",
    "\n",
    "# One Hot Encoding for categorical columns\n",
    "def one_hot_encode(dataframe, categorical_columns, drop_first=True):\n",
    "    dataframe = pd.get_dummies(dataframe, columns=categorical_columns, drop_first=drop_first)\n",
    "    return dataframe\n",
    "\n",
    "# Get list of categorical columns for one hot encoding\n",
    "ohe_columns = [col for col in df.columns if 10 >= df[col].nunique() > 2]\n",
    "\n",
    "# Apply one hot encoding to categorical columns\n",
    "df = one_hot_encode(df, ohe_columns)\n",
    "\n",
    "# Display the updated dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc807f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdced92c",
   "metadata": {},
   "source": [
    "## Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5834abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = df[['Insulin_CAT','BMI_CAT_Normal', 'BMI_CAT_Overweight', 'BMI_CAT_Obesity 1','BMI_CAT_Obesity 2', \n",
    "             'BMI_CAT_Obesity 3', 'BloodPressure_CAT_Normal','BloodPressure_CAT_Prehypertension', \n",
    "             'BloodPressure_CAT_Hypertension','BloodPressure_CAT_Hypertensive_Crisis', 'Glucose_CAT_Normal',\n",
    "             'Glucose_CAT_Overweight', 'Glucose_CAT_High', 'Age_CAT_Mature','Age_CAT_Old', 'Preg_CAT_One_Time', \n",
    "             'Preg_CAT_Many_Times']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00086548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the target variable 'Outcome' from the dataset\n",
    "y = df[\"Outcome\"]\n",
    "\n",
    "# Remove specific columns and create the feature matrix X\n",
    "X = df.drop(['Outcome', 'Insulin_CAT',\n",
    "       'BMI_CAT_Normal', 'BMI_CAT_Overweight', 'BMI_CAT_Obesity 1',\n",
    "       'BMI_CAT_Obesity 2', 'BMI_CAT_Obesity 3', 'BloodPressure_CAT_Normal',\n",
    "       'BloodPressure_CAT_Prehypertension', 'BloodPressure_CAT_Hypertension',\n",
    "       'BloodPressure_CAT_Hypertensive_Crisis', 'Glucose_CAT_Normal',\n",
    "       'Glucose_CAT_Overweight', 'Glucose_CAT_High', 'Age_CAT_Mature',\n",
    "       'Age_CAT_Old', 'Preg_CAT_One_Time', 'Preg_CAT_Many_Times'], axis=1)\n",
    "\n",
    "# Store the remaining column names in 'cols'\n",
    "cols = X.columns\n",
    "\n",
    "# Store the original index in 'index'\n",
    "index = X.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeda845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization of variables plays a vital role in enhancing model performance.\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Fit a RobustScaler transformer to the data\n",
    "transformer = RobustScaler().fit(X)\n",
    "\n",
    "# Transform the data using the fitted transformer\n",
    "X = transformer.transform(X)\n",
    "\n",
    "# Convert the transformed data back to a DataFrame with specified columns and index\n",
    "X = pd.DataFrame(X, columns=cols, index=index)\n",
    "X = pd.concat([X, cat_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d81de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af163d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeea916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes of X, X_train, and X_test\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e5ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifiers\n",
    "rf = RandomForestClassifier()\n",
    "svm = SVC()\n",
    "lr = LogisticRegression()\n",
    "knn = KNeighborsClassifier()\n",
    "XGB = GradientBoostingClassifier()\n",
    "dt = DecisionTreeClassifier()\n",
    "Ada = AdaBoostClassifier()\n",
    "lgbm = LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e612c0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('Logistic Regression', lr),\n",
    "    ('K-Nearest Neighbors', knn),\n",
    "    ('Decision Tree', dt),\n",
    "    ('Random Forest', rf),\n",
    "    ('Support Vector Machine', svm),\n",
    "    ('Gradient Boosting', XGB),\n",
    "    ('AdaBoost', Ada),\n",
    "    ('LightGBM', lgbm)\n",
    "]\n",
    "\n",
    "# Evaluate each model in the list\n",
    "results = []\n",
    "names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b2cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out LightGBM warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Found whitespace in feature_names\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"No further splits with positive gain\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Auto-choosing col-wise multi-threading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66f12be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models:\n",
    "    \n",
    "        kfold = KFold(n_splits = 10 )\n",
    "        cv_results = cross_val_score(model, X, y, cv = 10, scoring= \"accuracy\")\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        msg = \"%s: %.2f (%.2f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e4006c",
   "metadata": {},
   "source": [
    "## Model Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384902d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of classifiers and their respective parameter grids for hyperparameter tuning\n",
    "classifier_parameter_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    },\n",
    "    'K-Nearest Neighbors': {\n",
    "        'n_neighbors': [3, 5, 7],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Support Vector Machine': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1, 1],\n",
    "        'max_depth': [3, 4, 5]\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 1]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "       'n_estimators': [50, 100, 200],\n",
    "       'learning_rate': [0.01, 0.1, 0.2],\n",
    "       'max_depth': [3, 4, 5],\n",
    "       'subsample': [0.8, 1.0],\n",
    "       'colsample_bytree': [0.8, 1.0],\n",
    "       'reg_alpha': [0, 0.1, 0.5],\n",
    "       'reg_lambda': [0, 0.1, 0.5]\n",
    "    }\n",
    "}\n",
    "\n",
    "models = [\n",
    "    ('Logistic Regression', lr),\n",
    "    ('K-Nearest Neighbors', knn),\n",
    "    ('Decision Tree', dt),\n",
    "    ('Random Forest', rf),\n",
    "    ('Support Vector Machine', svm),\n",
    "    ('Gradient Boosting', XGB),\n",
    "    ('AdaBoost', Ada),\n",
    "    ('LightGBM', lgbm)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ee29b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a dictionary to store the best estimator for each classifier after hyperparameter tuning\n",
    "best_estimators = {}\n",
    "\n",
    "# Iterate over each classifier and their respective parameter grid\n",
    "for name, classifier in models:\n",
    "    # Retrieve the parameter grid for the current classifier\n",
    "    param_grid = classifier_parameter_grids[name]\n",
    "    \n",
    "    # Initialize GridSearchCV with the given classifier, parameter grid, and 5-fold cross-validation\n",
    "    grid_search = GridSearchCV(classifier, param_grid, cv=10)\n",
    "    \n",
    "    # Fit the GridSearchCV object on the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Print the best hyperparameters for the current classifier\n",
    "    print(f'Best hyperparameters for {name}: {grid_search.best_params_}')\n",
    "    \n",
    "    # Store the best estimator for the current classifier in the dictionary\n",
    "    best_estimators[name] = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfd76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a dictionary to store the tuned models\n",
    "tuned_models = {}\n",
    "\n",
    "# Iterate over the best_estimators dictionary\n",
    "for name, best_estimator in best_estimators.items():\n",
    "    # Fit the best estimator on the training data\n",
    "    best_estimator.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the testing data\n",
    "    y_pred = best_estimator.predict(X_test)\n",
    "    \n",
    "    # Calculate and print the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy of {name}: {accuracy:.2f}')\n",
    "    \n",
    "    # Store the trained model in the dictionary\n",
    "    tuned_models[name] = best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cbbb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predicted labels for test data using best models found for each classifier\n",
    "y_pred_lr = best_estimators['Logistic Regression'].predict(X_test)\n",
    "y_pred_knn = best_estimators['K-Nearest Neighbors'].predict(X_test)\n",
    "y_pred_dt = best_estimators['Decision Tree'].predict(X_test)\n",
    "y_pred_rf = best_estimators['Random Forest'].predict(X_test)\n",
    "y_pred_svm = best_estimators['Support Vector Machine'].predict(X_test)\n",
    "y_pred_gb = best_estimators['Gradient Boosting'].predict(X_test)\n",
    "y_pred_ada = best_estimators['AdaBoost'].predict(X_test)\n",
    "y_pred_lgbm = best_estimators['LightGBM'].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d7379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Model prediction accuracy\n",
    "lr_acc = round(accuracy_score(y_test, y_pred_lr)*100, 2)\n",
    "knn_acc = round(accuracy_score(y_test, y_pred_knn)*100, 2)\n",
    "dt_acc = round(accuracy_score(y_test, y_pred_dt)*100, 2)\n",
    "rf_acc = round(accuracy_score(y_test, y_pred_rf)*100, 2)\n",
    "svm_acc = round(accuracy_score(y_test, y_pred_svm)*100, 2)\n",
    "gb_acc = round(accuracy_score(y_test, y_pred_gb)*100, 2)\n",
    "ada_acc = round(accuracy_score(y_test, y_pred_ada)*100, 2)\n",
    "lgbm_acc = round(accuracy_score(y_test, y_pred_lgbm)*100, 2)\n",
    "\n",
    "print(f'Logistic Regression Accuracy {lr_acc}%')\n",
    "print(f'K-Nearest Neighbors Accuracy {knn_acc}%')\n",
    "print(f'Decision Tree Accuracy {dt_acc}%')\n",
    "print(f'Random Forest Accuracy {rf_acc}%')\n",
    "print(f'Support Vector Machine {svm_acc}%')\n",
    "print(f'Gradient Boosting {gb_acc}%')\n",
    "print(f'AdaBoost Accuracy {ada_acc}%')\n",
    "print(f'LightGBM Accuracy {lgbm_acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36116db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "# Create a dictionary to store the F1 scores and ROC AUC scores\n",
    "scores = {}\n",
    "\n",
    "# Iterate over the tuned_models dictionary\n",
    "for name, model in tuned_models.items():\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate the F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Calculate the ROC AUC score\n",
    "    if hasattr(model, 'predict_proba'):  # Check if the model supports predict_proba\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
    "        roc_auc = roc_auc_score(y_test, y_prob)\n",
    "    else:\n",
    "        roc_auc = None  # Set ROC AUC to None if predict_proba is not available\n",
    "    \n",
    "    # Store the F1 score and ROC AUC score in the scores dictionary\n",
    "    scores[name] = {'F1 Score': f1, 'ROC AUC Score': roc_auc}\n",
    "\n",
    "# Print the F1 scores and ROC AUC scores for each model\n",
    "for name, score in scores.items():\n",
    "    if score['ROC AUC Score'] is not None:\n",
    "        print(f'{name} - F1 Score: {score[\"F1 Score\"]:.2f}, ROC AUC Score: {score[\"ROC AUC Score\"]:.2f}')\n",
    "    else:\n",
    "        print(f'{name} - F1 Score: {score[\"F1 Score\"]:.2f}, ROC AUC Score: N/A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3e4588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Create a dictionary to store the ROC curves and AUC values\n",
    "roc_curves = {}\n",
    "\n",
    "# Create a figure and axis for the ROC curve plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Iterate over the tuned_models dictionary\n",
    "for name, model in tuned_models.items():\n",
    "    if hasattr(model, 'predict_proba'):  # Check if the model supports predict_proba\n",
    "        # Predict probabilities for the positive class\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculate the ROC curve\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        \n",
    "        # Calculate the AUC value\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        # Store the ROC curve and AUC value in the roc_curves dictionary\n",
    "        roc_curves[name] = {'fpr': fpr, 'tpr': tpr, 'roc_auc': roc_auc}\n",
    "        \n",
    "        # Plot the ROC curve\n",
    "        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "# Plot the diagonal reference line for random guessing\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1ecafd",
   "metadata": {},
   "source": [
    "Print the Confusion Matrix for all the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44f6942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the best estimator dictionary and print the confusion matrix and classification report for each classifier\n",
    "for name, model in best_estimators.items():\n",
    "    # Generate predicted labels for test data using the best model\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Print the confusion matrix and classification report for the current classifier\n",
    "    print(f'Confusion matrix for {name}:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print('\\n')\n",
    "    print(f'Classification report for {name}:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d896d5",
   "metadata": {},
   "source": [
    "Plot the Confussion Matrix Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b4def6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of rows and columns for subplots\n",
    "num_classifiers = len(best_estimators)\n",
    "num_cols = 2\n",
    "num_rows = int(np.ceil(num_classifiers / num_cols))\n",
    "\n",
    "# Create subplots for the confusion matrix heatmaps\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 6 * num_rows))\n",
    "fig.suptitle('Confusion Matrix Heatmaps')\n",
    "\n",
    "# Flatten the axes if there is only one row of subplots\n",
    "if num_rows == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Iterate over the best estimators dictionary and create a confusion matrix heatmap for each classifier\n",
    "for idx, (name, estimator) in enumerate(best_estimators.items()):\n",
    "    row = idx // num_cols\n",
    "    col = idx % num_cols\n",
    "    ax = axes[row][col]\n",
    "    \n",
    "    # Generate predicted labels for test data using the current classifier\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    \n",
    "    # Create a confusion matrix heatmap for the current classifier\n",
    "    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap=\"YlGnBu\", ax=ax)\n",
    "    ax.set_title(f'Confusion Matrix - {name}')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_ylabel('True Label')\n",
    "\n",
    "# Adjust layout and show the subplots\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44c351",
   "metadata": {},
   "source": [
    "### Models Comparism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab1a011",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model comparison \n",
    "models = pd.DataFrame({\n",
    "    'Model' : [ 'Logistics Regression','K-Nearest Neighbors', 'Decision Tree', 'Random Forest', 'Support Vector Machine','Gradient Boosting', 'Adaboost', 'LightGBM'],\n",
    "    'Score' : [ lr_acc, knn_acc, dt_acc, rf_acc, svm_acc, gb_acc, ada_acc, lgbm_acc ]\n",
    "})\n",
    "\n",
    "\n",
    "models.sort_values(by = 'Score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a88b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Acuracy for all the models\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(models['Model'], models['Score'], color='skyblue')\n",
    "plt.xlabel('Accuracy (%)')\n",
    "plt.ylabel('Model')\n",
    "plt.title('Model Comparison')\n",
    "plt.xlim(0, 100)  # Set x-axis limit to percentage scale\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the highest score on top\n",
    "\n",
    "# Add text labels on each bar with scores in percentage format\n",
    "for bar in bars:\n",
    "    plt.text(bar.get_width(), bar.get_y() + bar.get_height() / 2, f'{bar.get_width():.2f}%',\n",
    "             va='center', color='black', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b54913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the AdaBoost model on the training data\n",
    "ada_model= tuned_models['AdaBoost'].fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances from the AdaBoost model\n",
    "feature_importances = ada_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to store feature names and their importance scores\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "\n",
    "# Sort the DataFrame by importance in ascending order\n",
    "importance_df_sorted = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the sorted feature importance\n",
    "print(importance_df_sorted)\n",
    "\n",
    "# Plot the sorted feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df_sorted['Feature'], importance_df_sorted['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance for AdaBoost')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature on top\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dd18ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Decision model on the training data\n",
    "dt_model= tuned_models['Decision Tree'].fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances from the fitted model\n",
    "feature_importances = dt_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to store feature importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "\n",
    "# Sort the DataFrame by Importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print feature importance\n",
    "print(feature_importance_df)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance (Decision Tree)')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the highest importance on top\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645ae984",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
